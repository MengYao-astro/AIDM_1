{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AiDM assignment 1\n",
    "## Recommender system --- Matrix factorization \n",
    "\n",
    "## Read data\n",
    "We load the ratings data into a matrix which has 4 columns. The first column gives the user id. The second column gives the movie id. The third column gives the rating which can only be a integer from 1 to 5. The fourth column gives the timestamp in a unit of second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.genfromtxt('ml-1m/ratings.dat', delimiter= '::')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Next we create a list containing the unique user and movie ids, as well as a vector of 20 weights for each. We will train the network so that the dot product of a given user vector with a movie vector will be a prediction of the user's rating for that movie. In this way, the weights of a movie might reflect certain qualities whereas the weights of a user might reflect which qualities that particular user prefers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = np.unique(data[:,0])\n",
    "movie_list = np.unique(data[:,1])\n",
    "user_vector = np.random.uniform(size=(len(user_list),20))\n",
    "movie_vector = np.random.uniform(size=(len(movie_list),20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create vectors of the indicies for the weights vectors for the users and movies in the original data from ratings.dat. This is important since weight vectors for *unique* movies is not the same as the movie id, since there some movies which have not been rated in the data and it would be unhelpful to have random, unconstrained wieght vectors for movies not rated in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_locator = np.zeros(len(data))\n",
    "movie_locator = np.zeros(len(data))\n",
    "for i in range(len(data)):\n",
    "    user_locator[i] = np.where(user_list==data[i,0])[0][0]\n",
    "    movie_locator[i] = np.where(movie_list==data[i,1])[0][0]\n",
    "print(user_locator)\n",
    "print(movie_locator)\n",
    "print(data[:,1]) # Not the same! There are some unrated movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the weight vectors! Note that we force predictions to be between 1 and 5, and include a lambda regularization factor to counteract overfitting with large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate = 0.001\n",
    "lamb = 0.01\n",
    "iterations = 10\n",
    "total_error = np.zeros(iterations)\n",
    "for count in range(iterations):\n",
    "    for i in range(len(data)):\n",
    "        est_rating = np.dot(user_vector[int(user_locator[i])],movie_vector[int(movie_locator[i])])\n",
    "        if est_rating < 1 :\n",
    "            est_rating = 1\n",
    "        if est_rating > 5 :\n",
    "            est_rating = 5\n",
    "        error = data[i,2] - est_rating\n",
    "        user_vector[int(user_locator[i])]  += \\\n",
    "        lrate * (error * movie_vector[int(movie_locator[i])] - lamb *  user_vector[int(user_locator[i])])\n",
    "        movie_vector[int(movie_locator[i])] += \\\n",
    "        lrate * (error * user_vector[int(user_locator[i])] - lamb * movie_vector[int(movie_locator[i])])\n",
    "        total_error[count] += abs(error)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our network, let's take a look at how it predicts a few random entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.randint(len(data), size=10):\n",
    "    print(i, np.dot(user_vector[int(user_locator[i])],movie_vector[int(movie_locator[i])]), data[i,2], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Now lets see how the error changes over the course of fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to test changing the length of our weight vectors. However, adding more weights might just make our network better at over-fitting. We need to randomly seperate our data into a training and testing set. We can even split it into multiple parts, train models on each separately, and average our result for a prediction - N Fold Cross Validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
